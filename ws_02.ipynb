{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805733e7-7c38-4a5d-acb4-d94036d66869",
   "metadata": {},
   "source": [
    "# VSB,FEI - Generative AI Workshop\n",
    "\n",
    "The aim of the workshop is to get an overview of data analysis and deep learning techniques in the generative artificial intelligence (GenAI) domain.\n",
    "\n",
    "* We will use [Python](https://www.python.org/), [Huggingface](https://huggingface.co/) and [Tensorflow](https://www.tensorflow.org/).\n",
    "\n",
    "**The exercise will cover these topics:**\n",
    "* GenAI tools for image data using Huggingface models\n",
    "<!-- * LLM usage for text generating with Huggingface API -->\n",
    "* Vector representation of text data and searching for similar words using vector distance \n",
    "* Design of own deep learning model for generating \"Harry Potter\"-like text using Keras framework from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d389c-7f65-41a6-ab6f-16e39e60a3b6",
   "metadata": {},
   "source": [
    "## Deep learning in Python introduction\n",
    "* This lecture is focused on using word embedding for searching for similar words and RNN usage for text generation.\n",
    "\n",
    "* We will use Harry Potter books in this lectures for demonstration of training own model in Keras and generating our own HP-like stories.\n",
    "\n",
    "![meme01](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_meme_01.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581b84e-1fd2-4c1f-b15e-f500d881f829",
   "metadata": {},
   "source": [
    "## Import of the TensorFlow\n",
    "The main version of the TensorFlow (TF) is a in the Version package in the field VERSION Since the TensformFlow 2.0 everything was encapsulaed under the KERAS api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1315a0be-4e1e-4e99-b2bf-ac1f8d3510b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee48a52-15d4-4c40-9d28-8eabbe221abf",
   "metadata": {},
   "source": [
    "# ðŸ”Ž How does the neural network work with text?\n",
    "* Is is capable to process text directly or does it works just with numbers?\n",
    "* Can you come up with some very simple way how to encode text to numbers?\n",
    "\n",
    "# ðŸ”Ž What is a word embedding?\n",
    "* Why do we use it?\n",
    "* What different propeties will it have compared to some naive approaches?\n",
    "\n",
    "# Word embedding is a vector\n",
    "* Do you know what is vector?\n",
    "\n",
    "# $$\\vec{w} = \\left(w_1, w_2, ..., w_n\\right)$$\n",
    "\n",
    "![meme03](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_meme_tf_02.png?raw=true)\n",
    "\n",
    "# ðŸ’¡You can imagine embedding vector as an array of numbers, e.g. [0.5,0.3,0.1,-0.3,1.2]\n",
    "\n",
    "![meme03](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_05_enc_arch.png?raw=true)\n",
    "\n",
    "# The most famous word embedding is perhaps the Word2Vec\n",
    "\n",
    "## ðŸ’¡ There are two approaches for a Word2Vec embedding training\n",
    "\n",
    "* **Continuous bag-of-words model**: \n",
    "    * predicts the middle word based on surrounding context words. \n",
    "    * the context consists of a few words before and after the current (middle) word. \n",
    "    * this architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "\n",
    "* **Continuous skip-gram model**: \n",
    "    * predicts words within a certain range before and after the current word in the same sentence. \n",
    "\n",
    "![w2v](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_07_skip.png?raw=true)\n",
    "  \n",
    "* ðŸ’¡ Bag-of-words model predicts a word given the neighboring context\n",
    "* ðŸ’¡ Skip-gram model predicts the context (or neighbors) of a word, given the word itself\n",
    "\n",
    "* The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). \n",
    "* The context of a word can be represented through a set of skip-gram pairs of *(target_word, context_word)* where *context_word* appears in the neighboring context of target_word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d22c1-22b6-4a33-a7f6-fd63ba1a2a2c",
   "metadata": {},
   "source": [
    "## We will demonstrate the approach using single sentence\n",
    "\n",
    "* The context words for each of the 8 words of this sentence are defined by a window size. \n",
    "* The window size determines the span of words on either side of a target_word that can be considered a context word.\n",
    "\n",
    "![w2v_tab](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_07_tab.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05ab38-3d72-4373-b291-c066822e3fa2",
   "metadata": {},
   "source": [
    "# ðŸ’¡ The deep learning model de-facto learns which pairs of words are often appear together in text and which do not\n",
    "* Can you give some word-pairs examples yourself?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9de5c-a996-4bd6-9d96-e1b1e737a0c3",
   "metadata": {},
   "source": [
    "# A nice property of word embedding vectors is that vectors of similar meaning are put close together\n",
    "* If you compute a distance between them, it will be close to zero\n",
    "\n",
    "## Let's say that the vector is just 2D\n",
    "* How does 2D vector look like?\n",
    "* Can you calculate distance between two 2D vectors?\n",
    "* How is the formula called for 2D and how for n-D?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15052f6d-475b-42ef-958a-a5241a337c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ok, enough of theory!\n",
    "## Let's try it practically with a pre-trained vectors! ðŸ™‚\n",
    "* ðŸ”Ž Pre-trained on what!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbde4c09-e762-4e57-817a-1bf2ae46a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-06 15:36:13--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-09-06 15:36:14--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-09-06 15:36:15--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: â€˜glove.6B.zipâ€™\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.13MB/s    in 2m 40s  \n",
      "\n",
      "2023-09-06 15:38:56 (5.15 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdcc5240-266f-4191-900f-172e7cfaf647",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5f29b-ac7b-4a88-aeb0-140044c4df70",
   "metadata": {},
   "source": [
    "# We will take a look on the file structure now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e68898f4-756c-4aa9-9a83-e3752851fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      "\n",
      ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
      "\n",
      ". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n",
      "\n",
      "of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n",
      "\n",
      "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n",
      "\n",
      "and 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n",
      "\n",
      "in 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n",
      "\n",
      "a 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n",
      "\n",
      "\" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n",
      "\n",
      "'s 0.23727 0.40478 -0.20547 0.58805 0.65533 0.32867 -0.81964 -0.23236 0.27428 0.24265 0.054992 0.16296 -1.2555 -0.086437 0.44536 0.096561 -0.16519 0.058378 -0.38598 0.086977 0.0033869 0.55095 -0.77697 -0.62096 0.092948 -2.5685 -0.67739 0.10151 -0.48643 -0.057805 3.1859 -0.017554 -0.16138 0.055486 -0.25885 -0.33938 -0.19928 0.26049 0.10478 -0.55934 -0.12342 0.65961 -0.51802 -0.82995 -0.082739 0.28155 -0.423 -0.27378 -0.007901 -0.030231\n",
      "\n",
      "for 0.15272 0.36181 -0.22168 0.066051 0.13029 0.37075 -0.75874 -0.44722 0.22563 0.10208 0.054225 0.13494 -0.43052 -0.2134 0.56139 -0.21445 0.077974 0.10137 -0.51306 -0.40295 0.40639 0.23309 0.20696 -0.12668 -0.50634 -1.7131 0.077183 -0.39138 -0.10594 -0.23743 3.9552 0.66596 -0.61841 -0.3268 0.37021 0.25764 0.38977 0.27121 0.043024 -0.34322 0.020339 0.2142 0.044097 0.14003 -0.20079 0.074794 -0.36076 0.43382 -0.084617 0.1214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_glove_file) as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9bcde0-30a2-4f6f-902e-108b14193c28",
   "metadata": {},
   "source": [
    "# Let's load the file into a dictionary\n",
    "* key:value structure -> word:vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acfaa3a2-0fc9-401b-ba64-0db359ad18e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab38717-0561-41d8-abd6-ad7a650ff376",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ’¡ This is how the embedding latent vector looks like for the word 'audi' and 'bmw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b49cd48-4199-4222-aa90-67e760fc0fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.051355 ,  0.11694  ,  1.0251   ,  0.12414  , -0.83236  ,\n",
       "        1.0288   , -0.64566  , -1.4468   , -0.89265  , -0.32658  ,\n",
       "        0.66507  , -0.65524  , -1.8323   , -1.0347   ,  0.13486  ,\n",
       "       -0.033565 , -0.2208   ,  1.855    , -0.2495   , -0.84343  ,\n",
       "        0.14318  , -0.81258  , -0.84232  ,  1.1247   , -0.075604 ,\n",
       "       -0.30852  , -0.79071  ,  0.80721  , -0.24747  , -0.029263 ,\n",
       "        0.2684   ,  0.6531   ,  0.48872  ,  1.1838   ,  0.5606   ,\n",
       "       -0.68087  ,  0.25192  ,  0.98091  , -1.0433   , -0.27203  ,\n",
       "        1.1912   , -0.88594  ,  0.022038 , -0.82012  , -0.0022396,\n",
       "       -0.68251  ,  0.12713  ,  0.85041  ,  1.002    ,  0.33904  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['audi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f609faca-e38e-47e0-93b3-956e2800dac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70038 , -0.16073 ,  1.3423  ,  0.63331 , -0.21958 ,  0.31944 ,\n",
       "       -0.67042 , -0.94041 , -0.56935 , -0.67842 ,  0.39705 , -0.18964 ,\n",
       "       -2.2101  , -0.90947 ,  0.95511 , -0.01321 , -0.32738 ,  1.1554  ,\n",
       "       -0.48464 , -1.7606  , -0.051495, -1.0745  , -1.183   ,  0.68672 ,\n",
       "       -0.107   , -0.42152 , -0.15516 ,  0.12724 , -0.42114 ,  0.30905 ,\n",
       "        0.59784 ,  0.050149,  0.24022 ,  0.86494 ,  0.63488 , -0.75644 ,\n",
       "       -0.09189 ,  1.0218  , -0.96638 , -0.90508 ,  0.80575 , -0.75225 ,\n",
       "        0.7642  , -0.94425 ,  0.4609  ,  0.11877 ,  0.24907 ,  0.066667,\n",
       "        0.59622 ,  0.1275  ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['bmw']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e34bb-07c4-47fd-b769-0e4d691cdcee",
   "metadata": {},
   "source": [
    "## The cosine similarity of the car brands should be smaller than with some random word\n",
    "* Why?\n",
    "\n",
    "# Cosine vs. Euclidean similarity\n",
    "* ðŸ”Ž What is the difference?\n",
    "* ðŸ”Ž How to compute it?\n",
    "\n",
    "## $$cos(\\vec{A},\\vec{B}) = \\frac{\\sum_{i=1}^{n} A_i \\cdot B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2 \\cdot \\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0748bb6e-3b39-4aaa-ad3f-2de034983a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine(embeddings_index['audi'], embeddings_index['bmw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b46f90-68e5-4bdf-9bb7-25789ff92767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine(embeddings_index['audi'], embeddings_index['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ad08c-795b-43b0-b9cf-ada18aed1d63",
   "metadata": {},
   "source": [
    "# For trying the famous queen -> king example we need to build the embedding matrix\n",
    "\n",
    "![w2v_meme_03](https://github.com/rasvob/VSB-FEI-Deep-Learning-Exercises/blob/main/images/dl_07_meme_03.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3de1b9-627b-460d-914c-f67e0a0c96ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = len(embeddings_index.keys())\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "word2id = {k:i for i, (k,v) in enumerate(embeddings_index.items())}\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word2id.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e533c1-976b-4b36-b566-e4873a8e3f2e",
   "metadata": {},
   "source": [
    "## Finding the closest words is pretty easy now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c3c32-f55d-4e33-9fcc-670e6d12de5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_w = cosine_distances(embedding_matrix[word2id['man']].reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77afd32f-f442-4425-bb05-6182e6db8636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in c_w.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f539be-80c1-4aa3-bee1-ee72ea44f609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_w = cosine_distances(embedding_matrix[word2id['woman']].reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b369636-a3dc-40f2-bbed-579b61232b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in c_w.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a700643-51a6-457e-9bf7-c9e209bfe06f",
   "metadata": {},
   "source": [
    "## The idea is that using the difference between *man* and *woman* should be simillar as *king* and *queen* thus it should be possible to use the difference for searching for analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b2372-a2be-45da-849b-b3f752e6746e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist = embeddings_index['man'] - embeddings_index['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247f84b-6794-4563-8dce-fe35e40463eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fc70b-1383-41d6-8c2d-8be0607841fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summed = embeddings_index['queen'] + dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a2016-5505-4644-a6fe-f1249a136bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25921cc3-dd37-4782-82d9-52c0ed1d2195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = cosine_distances(summed.reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9013c45-b48d-41c5-bcf3-ee206d3d4bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d5527-642c-49fd-ae66-0948ef1a2cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a45ced-0c5b-47ad-a7dc-ced2919c7c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961ef15-cdb6-42fd-84d4-d0c5756e9621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da0f93-a738-4d96-8140-81147244d3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "502e48b7-8f01-499e-a463-b1dd3ec4c5de",
   "metadata": {},
   "source": [
    "![meme0_final](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/thats_all.jpg?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
