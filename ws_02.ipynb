{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805733e7-7c38-4a5d-acb4-d94036d66869",
   "metadata": {},
   "source": [
    "# VSB,FEI - Generative AI Workshop\n",
    "\n",
    "The aim of the workshop is to get an overview of data analysis and deep learning techniques in the generative artificial intelligence (GenAI) domain.\n",
    "\n",
    "* We will use [Python](https://www.python.org/), [Huggingface](https://huggingface.co/) and [Tensorflow](https://www.tensorflow.org/).\n",
    "\n",
    "**The exercise will cover these topics:**\n",
    "* GenAI tools for image data using Huggingface models\n",
    "<!-- * LLM usage for text generating with Huggingface API -->\n",
    "* Vector representation of text data and searching for similar words using vector distance \n",
    "* Design of own deep learning model for generating \"Harry Potter\"-like text using Keras framework from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d389c-7f65-41a6-ab6f-16e39e60a3b6",
   "metadata": {},
   "source": [
    "## âš› Deep learning in Python âš›\n",
    "* This lecture is focused on using word embedding for searching for similar words and RNN usage for text generation.\n",
    "\n",
    "* We will use Harry Potter books in this lectures for demonstration of training own model in Keras and generating our own HP-like stories.\n",
    "\n",
    "![meme01](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_meme_01.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581b84e-1fd2-4c1f-b15e-f500d881f829",
   "metadata": {},
   "source": [
    "## Import of the TensorFlow\n",
    "The main version of the TensorFlow (TF) is a in the Version package in the field VERSION Since the TensformFlow 2.0 everything was encapsulaed under the KERAS api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1315a0be-4e1e-4e99-b2bf-ac1f8d3510b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import unicodedata, re, string\n",
    "import nltk\n",
    "import requests\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from typing import List, Tuple\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional\n",
    "\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef8bf3-cd13-4950-ab18-b02b777f6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee48a52-15d4-4c40-9d28-8eabbe221abf",
   "metadata": {},
   "source": [
    "# ðŸ”Ž How does the neural network work with text?\n",
    "* Is is capable to process text directly or does it works just with numbers?\n",
    "* Can you come up with some very simple way how to encode text to numbers?\n",
    "\n",
    "# ðŸ”Ž What is a word embedding?\n",
    "* Why do we use it?\n",
    "* What different propeties will it have compared to some naive approaches?\n",
    "\n",
    "# Word embedding is a vector\n",
    "* Do you know what is vector?\n",
    "\n",
    "# $$\\vec{w} = \\left(w_1, w_2, ..., w_n\\right)$$\n",
    "\n",
    "# ðŸ’¡You can imagine embedding vector as an array of numbers, e.g. [0.5,0.3,0.1,-0.3,1.2]\n",
    "\n",
    "![meme03](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_05_enc_arch.png?raw=true)\n",
    "\n",
    "# The most famous word embedding is perhaps the Word2Vec\n",
    "\n",
    "## ðŸ’¡ There are two approaches for a Word2Vec embedding training\n",
    "\n",
    "* **Continuous bag-of-words model**: \n",
    "    * predicts the middle word based on surrounding context words. \n",
    "    * the context consists of a few words before and after the current (middle) word. \n",
    "    * this architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "\n",
    "* **Continuous skip-gram model**: \n",
    "    * predicts words within a certain range before and after the current word in the same sentence. \n",
    "\n",
    "![w2v](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_07_skip.png?raw=true)\n",
    "  \n",
    "* ðŸ’¡ Bag-of-words model predicts a word given the neighboring context\n",
    "* ðŸ’¡ Skip-gram model predicts the context (or neighbors) of a word, given the word itself\n",
    "* ðŸ’¡ The context of a word can be represented through a set of skip-gram pairs of *(target_word, context_word)* where *context_word* appears in the neighboring context of target_word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d22c1-22b6-4a33-a7f6-fd63ba1a2a2c",
   "metadata": {},
   "source": [
    "## We will demonstrate the approach using single sentence\n",
    "\n",
    "* The context words for each of the 8 words of this sentence are defined by a window size. \n",
    "* The window size determines the span of words on either side of a target_word that can be considered a context word.\n",
    "\n",
    "![w2v_tab](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_07_tab.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05ab38-3d72-4373-b291-c066822e3fa2",
   "metadata": {},
   "source": [
    "# ðŸ’¡ The deep learning model de-facto learns which pairs of words are often appear together in text and which do not\n",
    "* Can you give some word-pairs examples yourself?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9de5c-a996-4bd6-9d96-e1b1e737a0c3",
   "metadata": {},
   "source": [
    "# A nice property of word embedding vectors is that vectors of similar meaning are put close together\n",
    "* If you compute a distance between two similar words, it will be less than for two unrelated words\n",
    "* E.g. dog - animal X car - cake\n",
    "\n",
    "## Let's say that the vector is just 2D\n",
    "* How does 2D vector look like?\n",
    "* ðŸ”Ž Can you calculate distance between two 2D vectors?\n",
    "* ðŸ”Ž How is the formula called for 2D and how for n-D?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15052f6d-475b-42ef-958a-a5241a337c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ok, enough of theory!\n",
    "## Let's try it practically with a pre-trained vectors! ðŸ™‚\n",
    "* ðŸ”Ž Pre-trained on what!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde4c09-e762-4e57-817a-1bf2ae46a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc5240-266f-4191-900f-172e7cfaf647",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5f29b-ac7b-4a88-aeb0-140044c4df70",
   "metadata": {},
   "source": [
    "# We will take a look on the file structure now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68898f4-756c-4aa9-9a83-e3752851fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_glove_file) as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        i += 1\n",
    "        if i > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9bcde0-30a2-4f6f-902e-108b14193c28",
   "metadata": {},
   "source": [
    "# Let's load the file into a dictionary\n",
    "* key:value structure -> word:vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfaa3a2-0fc9-401b-ba64-0db359ad18e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab38717-0561-41d8-abd6-ad7a650ff376",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ’¡ This is how the embedding latent vector looks like for the word 'audi' and 'bmw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49cd48-4199-4222-aa90-67e760fc0fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_index['audi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609faca-e38e-47e0-93b3-956e2800dac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_index['bmw']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e34bb-07c4-47fd-b769-0e4d691cdcee",
   "metadata": {},
   "source": [
    "## ðŸ’¡ The cosine similarity of the car brands should be smaller than of some random words\n",
    "* Why?\n",
    "\n",
    "# Cosine vs. Euclidean similarity\n",
    "* ðŸ”Ž What is the difference?\n",
    "* ðŸ”Ž How to compute it?\n",
    "\n",
    "![meme03](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_meme_tf_02.png?raw=true)\n",
    "\n",
    "## $$cos(\\vec{A},\\vec{B}) = \\frac{\\sum_{i=1}^{n} A_i \\cdot B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2 \\cdot \\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe9e48b-0085-4491-9f84-08580e6596c0",
   "metadata": {},
   "source": [
    "# Let's try it out! ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0748bb6e-3b39-4aaa-ad3f-2de034983a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine(embeddings_index['audi'], embeddings_index['bmw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b46f90-68e5-4bdf-9bb7-25789ff92767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine(embeddings_index['audi'], embeddings_index['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ad08c-795b-43b0-b9cf-ada18aed1d63",
   "metadata": {},
   "source": [
    "# For trying the famous queen -> king example we need to build the embedding matrix\n",
    "\n",
    "![w2v_meme_03](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_07_meme_03.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3de1b9-627b-460d-914c-f67e0a0c96ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = len(embeddings_index.keys())\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "word2id = {k:i for i, (k,v) in enumerate(embeddings_index.items())}\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word2id.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e533c1-976b-4b36-b566-e4873a8e3f2e",
   "metadata": {},
   "source": [
    "## Finding the closest words is pretty easy now\n",
    "* ðŸ”Ž What is the distance for two same words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c3c32-f55d-4e33-9fcc-670e6d12de5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_w = cosine_distances(embedding_matrix[word2id['man']].reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77afd32f-f442-4425-bb05-6182e6db8636",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in c_w.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f539be-80c1-4aa3-bee1-ee72ea44f609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_w = cosine_distances(embedding_matrix[word2id['woman']].reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b369636-a3dc-40f2-bbed-579b61232b3d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in c_w.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a700643-51a6-457e-9bf7-c9e209bfe06f",
   "metadata": {},
   "source": [
    "## The idea is that using the difference between *man* and *woman* should be simillar as *king* and *queen* thus it should be possible to use the difference for searching for analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b2372-a2be-45da-849b-b3f752e6746e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist = embeddings_index['man'] - embeddings_index['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247f84b-6794-4563-8dce-fe35e40463eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fc70b-1383-41d6-8c2d-8be0607841fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summed = embeddings_index['queen'] + dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a2016-5505-4644-a6fe-f1249a136bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25921cc3-dd37-4782-82d9-52c0ed1d2195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = cosine_distances(summed.reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9013c45-b48d-41c5-bcf3-ee206d3d4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93baaac3-bdcc-46f6-8984-485384b0103d",
   "metadata": {},
   "source": [
    "# And here we go ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d5527-642c-49fd-ae66-0948ef1a2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in res.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30167ebe-7e0c-4917-92f0-f4c0d1c419b1",
   "metadata": {},
   "source": [
    "# âš› Deep learning usage in a text generation task âš›\n",
    "* We will use Harry Potter books in this lectures for generating our own stories.\n",
    "* 1st step is a data pre-processing so we transform the data into a form suitable for a deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd3164-db15-4036-bed1-26af0097a01c",
   "metadata": {},
   "source": [
    "# We need to download the data first and split text to lines\n",
    "* Download the text file using *requests* library\n",
    "* Convert raw HTTP response into a text and split it by lines into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da0f93-a738-4d96-8140-81147244d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get('https://raw.githubusercontent.com/rasvob/PopAI-VSB-Workshop/main/data/hp1.txt', allow_redirects=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da365f06-1b08-4162-914f-45269a8f0bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = str(req.text).splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d78b4-3d83-434b-96c0-fcd9021ab5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ccb54-a6a5-47bd-83dd-07745114216a",
   "metadata": {},
   "source": [
    "# Let's clean the data and do a brief exploration analysis after that ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587468e-c37e-484a-9b0b-6c8078832225",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Skip the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6723a-571d-4144-8e8e-aa0ee9b72d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt[3:]\n",
    "txt[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d91f54-a685-44ed-ac44-49cce7e9b1cd",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Remove the chapter header with chapter name\n",
    "We will remove the blank lines in this part as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65959c32-4aa4-4cd0-91b6-edc5683f380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [x for x in txt if 'CHAPTER ' not in x]\n",
    "txt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806639f-ddff-4898-b555-3937166fdb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [x for x in txt if not x.upper() == x]\n",
    "txt[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45accdef-51b7-4e61-82fd-24e30c3093a4",
   "metadata": {},
   "source": [
    "### ðŸ’¡ There are another minor imperfections connected to the  -- 't -- suffix, we need to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678aa22-d000-4ec1-ae38-cb0c956cc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in txt if \"\\'\" in x][25:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af611212-34ab-4265-ba81-1a93bfd2270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [x.replace('\"', '') for x in txt]\n",
    "[x for x in txt if \"a squeaky voice that\" in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec9204-773a-4022-9fb3-6463381ac391",
   "metadata": {},
   "source": [
    "### We will join the text to one long line and tokenize it after that\n",
    "* ðŸ’¡ We have prepared few useful functions that remove non-ASCII characters and fix some details in the text if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b250a-6679-47df-bdfe-26167aa43369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def fix_nt(words):\n",
    "    st_res = []\n",
    "    for i in range(0, len(words) - 1):\n",
    "        if words[i+1] == \"n't\" or words[i+1] == \"nt\":\n",
    "            st_res.append(words[i]+(\"n't\"))\n",
    "        else:\n",
    "            if words[i] != \"n't\" and words[i] != \"nt\":\n",
    "                st_res.append(words[i])\n",
    "    return st_res\n",
    "\n",
    "def fix_s(words):\n",
    "    st_res = []\n",
    "    for i in range(0, len(words) - 1):\n",
    "        if words[i+1] == \"'s\":\n",
    "            st_res.append(words[i]+(\"'s\"))\n",
    "        else:\n",
    "            if words[i] != \"'s\":\n",
    "                st_res.append(words[i])\n",
    "    return st_res\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = fix_nt(words)   \n",
    "    words = fix_s(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71968d-7d11-403f-b9d5-a2803dc8b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_one_line = ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d7d2a-781f-4e1f-b198-183116badba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_one_line[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5843d3-ba64-4955-b428-4db727383f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = TextBlob(txt_one_line).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa201b3-c9c7-45d9-b202-45bd6d6260f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = normalize(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f71d1c-37ed-4fce-b15d-a376bbd5789c",
   "metadata": {},
   "source": [
    "# ðŸ’¡ Let's take a look at the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e13c2-4352-4223-8d1c-4a106b2bd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = nltk.FreqDist(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094397f-06de-4508-ab5a-44b3275ac88d",
   "metadata": {},
   "source": [
    "## ðŸ’¡ We have 6829 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c72830-ee7c-4d32-abfe-0078b3b956d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456fcfa-d428-4bf4-9f3b-4043c10beac0",
   "metadata": {},
   "source": [
    "### These are the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc4432-e5dd-44ed-aa24-187d1afcaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = sorted(list(dist.items()), key=lambda x: x[1], reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b17478-e475-4231-b44e-1445363062cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots(1, figsize=(20, 14))\n",
    "sns.barplot(x=[x[0] for x in most_common_words], y=[x[1] for x in most_common_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c084f-d714-4903-926a-45954c675275",
   "metadata": {},
   "source": [
    "## ðŸ’¡ We have ~ 78300 words in the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a25cd9-4646-45f9-8a67-7ca6771b07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8197b-bb3b-47d9-aaad-4a506d016b6f",
   "metadata": {},
   "source": [
    "# ðŸ”Ž What kind of words are the most frequent? Is this information helpful?\n",
    "\n",
    "## Another nice visualization is a **WordCloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb199fd0-e8d2-4ac0-9737-9bd1424fa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(txt_one_line)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4416f-d5f3-4898-8693-ad4f6c294cb0",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Limit *max_font_size* if you want to include more words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3bd71-11c3-42c6-b40b-560f264cb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=40).generate(txt_one_line)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b363a22-2781-4e2e-9125-9e0c6bca48d2",
   "metadata": {},
   "source": [
    "# We can now learn how to use ANN as a text generator! ðŸ™‚\n",
    "* There are two main ways for solving the task\n",
    "    * Word-based model\n",
    "    * Character-based model\n",
    "\n",
    "## ðŸ”Ž How does it work from high-level view?\n",
    "    \n",
    "* ðŸ’¡ We have relatively small dataset thus we will use the **Character-based model** as it works better with smaller datasets\n",
    "* ðŸ’¡ We will also simplify the task for using only lower case letters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1a9bf-852b-4564-a0cc-c8641d22eacb",
   "metadata": {},
   "source": [
    "# ðŸ’¡ Build an array of letters from the whole text and filter out everything which is not lower-case letters and spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e0b8f-cc4b-4d33-b200-e961a70ec417",
   "metadata": {},
   "source": [
    "### Original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b0d0b-ab70-4802-b4f1-792ad09cdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_one_line[:240]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94512bfe-3ff9-4cba-b439-50a08ab33395",
   "metadata": {},
   "source": [
    "### Transform to lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c0c7f-8886-455a-b399-64d2b46e1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_one_line = txt_one_line.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910aa870-fef7-456e-ab5e-2029a2af26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_one_line[:240]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9900fa4-60b4-4e61-8e36-a2d197da4296",
   "metadata": {},
   "source": [
    "## Split into letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8161179-f316-4e2f-8eff-b2b33d59d391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "letters = []\n",
    "for x in txt_one_line:\n",
    "    if x >= 'a' and x <= 'z' or x == ' ':\n",
    "        letters.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb5c5a-7341-41a7-bd6d-42cb54212a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "letters[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e7709-9e55-4dfa-933e-81faee052c3b",
   "metadata": {},
   "source": [
    "# ðŸ’¡We have corpus of more than 400k characters available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28764f-9e8a-41c6-bb5e-79b63d3ac42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c34a9-d73d-467e-bfbf-9bad0a530954",
   "metadata": {},
   "source": [
    "## ðŸ’¡ But only 27 unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076face3-303c-4af4-b28f-0d07d4221881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(letters)))\n",
    "print(\"Total chars:\", len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad192d33-627f-4d0f-8878-7aaba7e861b6",
   "metadata": {},
   "source": [
    "## We will now build ID -> CHAR and CHAR -> ID lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b09a91-57be-4d56-b6fe-edfc83b3b1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45bde1-6f66-40fb-b526-4899255b7889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "char_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df66f2-8b6b-4aa9-a650-34cfc688714d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd75938-1b02-4759-8575-e58a1e2ef791",
   "metadata": {},
   "source": [
    "# We need to create fixed length sequences for the model\n",
    "* We will shift the sliding window of *SEQ_LEN* by *step* and for X,y pair\n",
    "* Input is array of *SEQ_LEN* letters output is just **1** letter which comes after the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123025a2-b6aa-41a3-9436-193f5233bd15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 40\n",
    "step = 1\n",
    "X, y = [], []\n",
    "for i in range(0, len(letters) - SEQ_LEN, step):\n",
    "    seq, ch = letters[i:i+SEQ_LEN], letters[i + SEQ_LEN]\n",
    "    X.append(seq)\n",
    "    y.append(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab469b-1ecd-4455-adba-bf58c395ffa6",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Let's take a look at the example\n",
    "* Focus on the last letter of the second sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb599f5a-6d62-4a72-b82f-fb68271f3323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcda867-45af-4d82-8fc5-690184628967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7e429-b94a-4a10-970b-f8eac05cee8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357834f-60fb-4e7d-a956-fd19f03ec7c2",
   "metadata": {},
   "source": [
    "# Characted level RNN uses usually one-hot encoding as we work just with a few unique tokens so no complex embedding is needed\n",
    "* ðŸ”Ž How would one-hot encoding look like for 4 letters A B C D?\n",
    "    * How many bits do we need? \n",
    "    * Can it be even more simplified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1d2d6-9584-414e-8181-af7107fc37b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_ohe = np.zeros((len(X), SEQ_LEN, len(chars)), dtype=bool)\n",
    "y_ohe = np.zeros((len(X), len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(X):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X_ohe[i, t, char_indices[char]] = 1\n",
    "    y_ohe[i, char_indices[y[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf08744-767c-4bf2-9b0a-39839cc938d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9d918-3c78-4228-9478-313b7f3f4337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_ohe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223bc992-2366-42d5-9357-53bea20252ec",
   "metadata": {},
   "source": [
    "# Final step is the model definition and training ðŸ™‚\n",
    "* What do we need the model to learn?\n",
    "* How does the model learn?\n",
    "* What is an input and what is an output?\n",
    "* We will use *softmax* function as an output\n",
    "    * How many neurons do we need at the ouput layer?\n",
    "\n",
    "* We use several types of layers\n",
    "    * ðŸ”Ž Have you heard about LSTM, Dense or Dropout layers yet?\n",
    "    * What about optimization algorithms? What is their purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a05d1-ebca-4122-88a3-8fb4f86bf42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(SEQ_LEN, len(chars)))\n",
    "x = LSTM(128, return_sequences=True)(input_layer)\n",
    "x = LSTM(128, return_sequences=False)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, 'relu')(x)\n",
    "x = keras.layers.Dense(128, 'relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "output_layer = keras.layers.Dense(len(chars), activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(input_layer, output_layer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=keras.losses.CategoricalCrossentropy(from_logits=False), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ccdf2-c0b5-4ebe-beff-e533ca68237e",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What is a **batch** and an **epoch**?\n",
    "* What is **loss function**?\n",
    "* What is *ModelCheckpoint* and why is it useful?\n",
    "    * ðŸ’¡ Hint: Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093c148-6cad-4374-be86-b1d09704e6a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.tf',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_ohe, y_ohe, validation_split=0.2, callbacks=[model_checkpoint_callback], epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42738c-cbe8-426c-a113-47b6c25a69be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392ecf8-8e7b-4f7f-b7fb-c4b66f637fdc",
   "metadata": {},
   "source": [
    "## Try to predict one letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada685ce-3cd1-41dd-b751-f2936635ce3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_ohe[0].reshape((1, 40, 27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae0874-c35d-4503-b5f3-fdb825902be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_ohe[0].reshape((1, 40, 27)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d81f2-6526-4148-8196-776f3a24f7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb09acea-f748-4063-910e-4a491a2dd024",
   "metadata": {},
   "source": [
    "# We won't use probabilities directly but we will sample from the predicted outputs using Temperature Softmax [see this](https://medium.com/@majid.ghafouri/why-should-we-use-temperature-in-softmax-3709f4e0161)\n",
    "\n",
    "* Basically, the ideas is that it would re-weight the probability distribution so that you can control how much surprising (i.e. higher temperature/entropy) or predictable (i.e. lower temperature/entropy) the next selected character would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f5723-cbfd-438f-aeb1-680c78cde58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59ef0b-b446-4250-932b-de17f91d679f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = sample(y_pred)\n",
    "indices_char[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca955e-202b-4305-a25d-c301c58cb23d",
   "metadata": {},
   "source": [
    "# And in the end we are able to create a feedback loop and use the model as a next characted generator for given seed text ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826d3c6-21b3-4eb0-9645-ede557d964b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "whole_text = X[10].copy()\n",
    "seq = X[10].copy()\n",
    "for i in range(500):\n",
    "    paragraph_ohe = np.zeros((1, SEQ_LEN, len(chars)))\n",
    "    for t, char in enumerate(seq):\n",
    "        paragraph_ohe[0, t, char_indices[char]] = 1\n",
    "    y_pred = model.predict(paragraph_ohe)\n",
    "    c = sample(y_pred[0], 0.5)\n",
    "    next_char = indices_char[c]\n",
    "    whole_text.append(next_char)\n",
    "    seq = whole_text[-SEQ_LEN:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be68181-9dae-4d40-997a-973c76ceaf51",
   "metadata": {},
   "source": [
    "## You can see that the model has only seen character-level data however it has learnt the patterns from th data thus it is able to generate existing words/phrases \n",
    "\n",
    "### And yes, the output is still far from ideal ðŸ™‚\n",
    "* ðŸ”Ž How would you make it better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0dd43f-be70-4a08-a8ac-9a2e1d5a4c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''.join(whole_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e48b7-8f01-499e-a463-b1dd3ec4c5de",
   "metadata": {},
   "source": [
    "![meme0_final](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/thats_all.jpg?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
