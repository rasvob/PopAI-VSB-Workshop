{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805733e7-7c38-4a5d-acb4-d94036d66869",
   "metadata": {},
   "source": [
    "# VSB,FEI - Generative AI Workshop\n",
    "\n",
    "The aim of the workshop is to get an overview of data analysis and deep learning techniques in the generative artificial intelligence (GenAI) domain.\n",
    "\n",
    "* We will use [Python](https://www.python.org/), [Huggingface](https://huggingface.co/) and [Tensorflow](https://www.tensorflow.org/).\n",
    "\n",
    "**The exercise will cover these topics:**\n",
    "* GenAI tools for image data using Huggingface models\n",
    "<!-- * LLM usage for text generating with Huggingface API -->\n",
    "* Vector representation of text data and searching for similar words using vector distance \n",
    "* Design of own deep learning model for generating \"Harry Potter\"-like text using Keras framework from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d389c-7f65-41a6-ab6f-16e39e60a3b6",
   "metadata": {},
   "source": [
    "## Deep learning in Python introduction\n",
    "* This lecture is focused on using word embedding for searching for similar words and RNN usage for text generation.\n",
    "\n",
    "* We will use Harry Potter books in this lectures for demonstration of training own model in Keras and generating our own HP-like stories.\n",
    "\n",
    "![meme01](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_meme_01.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581b84e-1fd2-4c1f-b15e-f500d881f829",
   "metadata": {},
   "source": [
    "## Import of the TensorFlow\n",
    "The main version of the TensorFlow (TF) is a in the Version package in the field VERSION Since the TensformFlow 2.0 everything was encapsulaed under the KERAS api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1315a0be-4e1e-4e99-b2bf-ac1f8d3510b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee48a52-15d4-4c40-9d28-8eabbe221abf",
   "metadata": {},
   "source": [
    "# ðŸ”Ž How does the neural network work with text?\n",
    "* Is is capable to process text directly or does it works just with numbers?\n",
    "* Can you come up with some very simple way how to encode text to numbers?\n",
    "\n",
    "# ðŸ”Ž What is a word embedding?\n",
    "* Why do we use it?\n",
    "* What different propeties will it have compared to some naive approaches?\n",
    "\n",
    "# Word embedding is a vector\n",
    "* Do you know what is vector?\n",
    "\n",
    "# $$\\vec{w} = \\left(w_1, w_2, ..., w_n\\right)$$\n",
    "\n",
    "# ðŸ’¡You can imagine embedding vector as an array of numbers, e.g. [0.5,0.3,0.1,-0.3,1.2]\n",
    "\n",
    "![meme03](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_05_enc_arch.png?raw=true)\n",
    "\n",
    "# The most famous word embedding is perhaps the Word2Vec\n",
    "\n",
    "## ðŸ’¡ There are two approaches for a Word2Vec embedding training\n",
    "\n",
    "* **Continuous bag-of-words model**: \n",
    "    * predicts the middle word based on surrounding context words. \n",
    "    * the context consists of a few words before and after the current (middle) word. \n",
    "    * this architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "\n",
    "* **Continuous skip-gram model**: \n",
    "    * predicts words within a certain range before and after the current word in the same sentence. \n",
    "\n",
    "![w2v](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_07_skip.png?raw=true)\n",
    "  \n",
    "* ðŸ’¡ Bag-of-words model predicts a word given the neighboring context\n",
    "* ðŸ’¡ Skip-gram model predicts the context (or neighbors) of a word, given the word itself\n",
    "* ðŸ’¡ The context of a word can be represented through a set of skip-gram pairs of *(target_word, context_word)* where *context_word* appears in the neighboring context of target_word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d22c1-22b6-4a33-a7f6-fd63ba1a2a2c",
   "metadata": {},
   "source": [
    "## We will demonstrate the approach using single sentence\n",
    "\n",
    "* The context words for each of the 8 words of this sentence are defined by a window size. \n",
    "* The window size determines the span of words on either side of a target_word that can be considered a context word.\n",
    "\n",
    "![w2v_tab](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_07_tab.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05ab38-3d72-4373-b291-c066822e3fa2",
   "metadata": {},
   "source": [
    "# ðŸ’¡ The deep learning model de-facto learns which pairs of words are often appear together in text and which do not\n",
    "* Can you give some word-pairs examples yourself?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9de5c-a996-4bd6-9d96-e1b1e737a0c3",
   "metadata": {},
   "source": [
    "# A nice property of word embedding vectors is that vectors of similar meaning are put close together\n",
    "* If you compute a distance between two similar words, it will be less than for two unrelated words\n",
    "* E.g. dog - animal X car - cake\n",
    "\n",
    "## Let's say that the vector is just 2D\n",
    "* How does 2D vector look like?\n",
    "* ðŸ”Ž Can you calculate distance between two 2D vectors?\n",
    "* ðŸ”Ž How is the formula called for 2D and how for n-D?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15052f6d-475b-42ef-958a-a5241a337c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ok, enough of theory!\n",
    "## Let's try it practically with a pre-trained vectors! ðŸ™‚\n",
    "* ðŸ”Ž Pre-trained on what!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde4c09-e762-4e57-817a-1bf2ae46a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc5240-266f-4191-900f-172e7cfaf647",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5f29b-ac7b-4a88-aeb0-140044c4df70",
   "metadata": {},
   "source": [
    "# We will take a look on the file structure now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68898f4-756c-4aa9-9a83-e3752851fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_glove_file) as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        i += 1\n",
    "        if i > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9bcde0-30a2-4f6f-902e-108b14193c28",
   "metadata": {},
   "source": [
    "# Let's load the file into a dictionary\n",
    "* key:value structure -> word:vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfaa3a2-0fc9-401b-ba64-0db359ad18e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab38717-0561-41d8-abd6-ad7a650ff376",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ’¡ This is how the embedding latent vector looks like for the word 'audi' and 'bmw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49cd48-4199-4222-aa90-67e760fc0fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_index['audi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609faca-e38e-47e0-93b3-956e2800dac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_index['bmw']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e34bb-07c4-47fd-b769-0e4d691cdcee",
   "metadata": {},
   "source": [
    "## ðŸ’¡ The cosine similarity of the car brands should be smaller than of some random words\n",
    "* Why?\n",
    "\n",
    "# Cosine vs. Euclidean similarity\n",
    "* ðŸ”Ž What is the difference?\n",
    "* ðŸ”Ž How to compute it?\n",
    "\n",
    "![meme03](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_meme_tf_02.png?raw=true)\n",
    "\n",
    "## $$cos(\\vec{A},\\vec{B}) = \\frac{\\sum_{i=1}^{n} A_i \\cdot B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2 \\cdot \\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe9e48b-0085-4491-9f84-08580e6596c0",
   "metadata": {},
   "source": [
    "# Let's try it out! ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0748bb6e-3b39-4aaa-ad3f-2de034983a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine(embeddings_index['audi'], embeddings_index['bmw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b46f90-68e5-4bdf-9bb7-25789ff92767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine(embeddings_index['audi'], embeddings_index['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ad08c-795b-43b0-b9cf-ada18aed1d63",
   "metadata": {},
   "source": [
    "# For trying the famous queen -> king example we need to build the embedding matrix\n",
    "\n",
    "![w2v_meme_03](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/dl_07_meme_03.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3de1b9-627b-460d-914c-f67e0a0c96ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = len(embeddings_index.keys())\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "word2id = {k:i for i, (k,v) in enumerate(embeddings_index.items())}\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word2id.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e533c1-976b-4b36-b566-e4873a8e3f2e",
   "metadata": {},
   "source": [
    "## Finding the closest words is pretty easy now\n",
    "* ðŸ”Ž What is the distance for two same words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c3c32-f55d-4e33-9fcc-670e6d12de5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_w = cosine_distances(embedding_matrix[word2id['man']].reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77afd32f-f442-4425-bb05-6182e6db8636",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in c_w.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f539be-80c1-4aa3-bee1-ee72ea44f609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_w = cosine_distances(embedding_matrix[word2id['woman']].reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b369636-a3dc-40f2-bbed-579b61232b3d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in c_w.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a700643-51a6-457e-9bf7-c9e209bfe06f",
   "metadata": {},
   "source": [
    "## The idea is that using the difference between *man* and *woman* should be simillar as *king* and *queen* thus it should be possible to use the difference for searching for analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b2372-a2be-45da-849b-b3f752e6746e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist = embeddings_index['man'] - embeddings_index['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247f84b-6794-4563-8dce-fe35e40463eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fc70b-1383-41d6-8c2d-8be0607841fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summed = embeddings_index['queen'] + dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a2016-5505-4644-a6fe-f1249a136bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25921cc3-dd37-4782-82d9-52c0ed1d2195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = cosine_distances(summed.reshape(-1, 50), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9013c45-b48d-41c5-bcf3-ee206d3d4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93baaac3-bdcc-46f6-8984-485384b0103d",
   "metadata": {},
   "source": [
    "# And here we go ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d5527-642c-49fd-ae66-0948ef1a2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in res.argsort().ravel()[1:6]:\n",
    "    print(id2word[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30167ebe-7e0c-4917-92f0-f4c0d1c419b1",
   "metadata": {},
   "source": [
    "# Deep learning usage in a text-based generative task\n",
    "* We will use Harry Potter books in this lectures for generating our own stories.\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961ef15-cdb6-42fd-84d4-d0c5756e9621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da0f93-a738-4d96-8140-81147244d3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "502e48b7-8f01-499e-a463-b1dd3ec4c5de",
   "metadata": {},
   "source": [
    "![meme0_final](https://github.com/rasvob/PopAI-VSB-Workshop/blob/main/images/thats_all.jpg?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
